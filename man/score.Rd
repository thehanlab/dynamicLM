% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/score.R
\name{score}
\alias{score}
\title{Methods (time-dependent AUC and Brier Score) to score the predictive
performance of dynamic risk prediction landmark models.}
\usage{
score(
  object,
  times,
  metrics = c("auc", "brier"),
  formula,
  data,
  lms = "LM",
  id_col,
  se.fit = TRUE,
  conf.int = 0.95,
  contrasts = TRUE,
  split.method = "none",
  B = 1,
  M,
  summary = TRUE,
  cores = 1,
  seed,
  cause,
  silent = TRUE,
  ...
)
}
\arguments{
\item{object}{A named list of prediction models, where allowed entries are
outputs from \code{\link[=predict.dynamicLM]{predict.dynamicLM()}} or supermodels from \code{\link[=dynamic_lm]{dynamic_lm()}}
depending on the type of calibration.}

\item{times}{Landmark times for which metrics should be calculated.
These must be a subset of landmark times used during the prediction.}

\item{metrics}{Character vector specifying which metrics to apply. Choices
are "auc" and "brier".}

\item{formula}{A survival or event history formula
(\code{\link[prodlim:Hist]{prodlim::Hist()}}). The left hand side is used to compute the
expected event status. If none is given, it is obtained from the prediction
object.}

\item{data}{Data for external validation.}

\item{lms}{Landmark times corresponding to the patient entries in data. Only
required if data is specified and is a dataframe.
\code{lms} can be a string (indicating a column in data), a vector of length
nrow(data), or a single value if all patient entries were obtained at the
same landmark time.}

\item{id_col}{Column name that identifies individuals in data. If omitted, it
is obtained from the prediction object.}

\item{se.fit}{If FALSE or 0, no standard errors are calculated.}

\item{conf.int}{Confidence interval (CI) coverage. Default is 0.95. If
bootstrapping, CIs are calculated from empirical quantiles. If not, for
right censored data, they are calculated by the package \link{riskRegression} as
in Blanche et al (references).}

\item{contrasts}{If TRUE, perform model comparison tests.}

\item{split.method}{Defines the internal validation design. Options are
currently "none" or "bootcv".

"none": assess the model in the test data (\code{data} argument)/data it was
trained on.

"bootcv": \code{B} models are trained on boostrap samples either drawn with
replacement of the same size as the original data or without replacement
of size \code{M}. Models are then assessed in observations not in the sample.}

\item{B}{Number of times bootstrapping is performed.}

\item{M}{Subsample size for training in cross-validation. Entries not sampled
in the M subsamples are used for validation.}

\item{summary}{Compute the summary metrics (average of the time-dependent
metrics). By default is TRUE.}

\item{cores}{To perform parallel computing, specifies the number of cores.
(Not yet implemented)}

\item{seed}{Optional, integer passed to set.seed. If not given or NA, no seed
is set.}

\item{cause}{Cause of interest if considering competing risks. If left blank,
this is inferred from object.}

\item{silent}{Show any error messages when computing \code{score} for each
landmark time (and potentially bootstrap iteration)}

\item{...}{Additional arguments to pass to \code{\link[riskRegression:Score]{riskRegression::Score()}}.
These arguments have been included for user flexibility but have not been
tested and should be used with precaution.}
}
\value{
An list with entries \code{AUC} and \code{Brier} if "auc" and "brier" were
included as metrics respectively and \code{AUC_Summary} and/or \code{Brier_summary}
if \code{summary} is not null. Each will have entries:
\itemize{
\item \code{score}: data.table containing the metric
\item \code{contrasts}: data.table containing model comparisons
}
}
\description{
There are three ways to perform assess the predictive performance:
apparent/internal, bootstrapped, and external. Accordingly, the named list of
prediction models must be as follows:
\itemize{
\item For both apparent/internal evaluation, objects output from
\code{\link[=predict.dynamicLM]{predict.dynamicLM()}} or supermodels fit with \code{\link[=dynamic_lm]{dynamic_lm()}} may be used as
input.
\item In order to bootstrap, supermodels fit with \code{\link[=dynamic_lm]{dynamic_lm()}} may be used as
input (note that the argument \code{x=TRUE} must be specified when fitting the
model in \code{\link[=dynamic_lm]{dynamic_lm()}}).
\item For external calibration, supermodels fit with \code{\link[=dynamic_lm]{dynamic_lm()}} are input
along with new data in the \code{data} argument. This data can be a LMdataframe
or a dataframe (in which case \code{lms} must be specified).
}
}
\details{
For both internal evaluation and bootstrapping, it is assumed that all
models in \code{object} are fit on the same data.

If data at late landmark times is sparse, some bootstrap samples may
not have patients that live long enough to perform evaluation leading to
the message "Upper limit of followup in bootstrap samples, was too low.
Results at evaluation time(s) beyond these points could not be computed
and are left as NA". In this case, consider only evaluating for earlier
landmarks or performing prediction with a smaller window as data points are
slim. If you wish to see which model/bootstrap/landmark times failed, set
SILENT=FALSE. Currently ignores these bootstraps and calculates
metrics from the bootstrap samples that worked.

Another message may occur: "Dropping bootstrap b = ... for model ... due
to unreliable predictions". As certain approximations are made, numerical
overflow sometimes occurs in predictions for bootstrapped samples. To avoid
potential errors, the whole bootstrap sample is dropped in this case. Note
that input data should be complete otherwise this may occur
unintentionally.
}
\examples{
\dontrun{
# Internal validation (using model)
scores <- score(list("Model1" = supermodel))
print(scores)

par(mfrow=c(1, 4))
plot(scores)

# Internal validation (using predictions)
p1 <- predict(supermodel)
scores <- score(list("Model1" = p1))
print(scores)


# # Bootstrapping
# Remember to fit the supermodel with argument 'x = TRUE'
scores <- score(list("Model1" = supermodel),
                split.method = "bootcv", B = 10) # 10 bootstraps
print(scores)

# External validation
# a) newdata is a dataframe
newdata <- relapse[relapse$T_txgiven == 0, ]
newdata$age <- newdata$age.at.time.0
newdata$LM <- 0
score(list("Model1" = supermodel), data = newdata, lms = "LM")

# b) newdata is a landmark dataset
lmdata_new <- lmdata
score(list("Model1" = supermodel), data = lmdata_new)
}

}
\references{
Paul Blanche, Cecile Proust-Lima, Lucie Loubere, Claudine Berr,
Jean- Francois Dartigues, and Helene Jacqmin-Gadda. Quantifying and
comparing dynamic predictive accuracy of joint models for longitudinal
marker and time-to-event in presence of censoring and competing risks.
Biometrics, 71 (1):102–113, 2015.

P. Blanche, J-F Dartigues, and H. Jacqmin-Gadda. Estimating and comparing
time-dependent areas under receiver operating characteristic curves for
censored event times with competing risks. Statistics in Medicine,
32(30):5381–5397, 2013.
}
