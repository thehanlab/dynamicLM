% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/score.R
\name{score}
\alias{score}
\title{Methods (time-dependent AUC and Brier Score) to score the predictive
performance of dynamic risk prediction landmark models.}
\usage{
score(
  object,
  times,
  metrics = c("auc", "brier"),
  formula,
  data,
  lms = "LM",
  id_col = "ID",
  se.fit = TRUE,
  conf.int = 0.95,
  split.method = "none",
  B = 1,
  M,
  cores = 1,
  seed,
  cause,
  silent = T,
  na.rm = FALSE,
  ...
)
}
\arguments{
\item{object}{A named list of prediction models, where allowed entries are
outputs from \code{\link[=predict.dynamicLM]{predict.dynamicLM()}} or supermodels from \code{\link[=dynamic_lm]{dynamic_lm()}} depending on the
type of calibration.}

\item{times}{Landmark times for which calibration must be plot. These must be
a subset of landmark times used during the prediction}

\item{metrics}{Character vector specifying which metrics to apply. Choices
are "auc" and "brier". Case matters.}

\item{formula}{A survival or event history formula
(\code{\link[prodlim:Hist]{prodlim::Hist()}}). The left hand side is used to compute the
expected event status. If none is given, it is obtained from the prediction
object.}

\item{data}{Data for external validation.}

\item{lms}{Landmark times corresponding to the patient entries in data. Only
required if data is specified and is a dataframe.
\code{lms} can be a string (indicating a column in data), a vector of length
nrow(data), or a single value if all patient entries were obtained at the
same landmark time.}

\item{id_col}{Column name that identifies individuals in data. If omitted, it
is obtained from the prediction object.}

\item{se.fit}{If FALSE or 0, no standard errors are calculated.}

\item{conf.int}{Confidence interval (CI) coverage. Default is 0.95. If
bootstrapping, CIs are calculated from empirical quantiles. If not, for
right censored data, they are calculated by the package \link{riskRegression} as
in Blanche et al (references).}

\item{split.method}{Defines the internal validation design. Options are
currently "none" or "bootcv".

"none": assess the model in the test data (\code{data} argument)/data it was
trained on.

"bootcv": \code{B} models are trained on boostrap samples either drawn with
replacement of the same size as the original data or without replacement
of size \code{M}. Models are then assessed in observations not in the sample.}

\item{B}{Number of times bootstrapping is performed.}

\item{M}{Subsample size for training in cross-validation. Entries not sampled
in the M subsamples are used for validation.}

\item{cores}{To perform parallel computing, specifies the number of cores.
(Not yet implemented)}

\item{seed}{Optional, integer passed to set.seed. If not given or NA, no seed
is set.}

\item{cause}{Cause of interest if considering competing risks. If left blank,
this is inferred from object.}

\item{silent}{Show any error messages when computing \code{score} for each
landmark time (and potentially bootstrap iteration)}

\item{na.rm}{Ignore bootstraps where there are errors (for example not
enough datasamples) and calculate metrics on remaining values. This is not
recommended. For example, if only one bootstrap sampling has enough data
that live to the prediction window, the standard error will be zero.}

\item{...}{Additional arguments to pass to \code{\link[riskRegression:Score]{riskRegression::Score()}}.
These arguments have been included for user flexibility but have not been
tested and should be used with precaution.}
}
\value{
An object of class "LMScore", which has components:
\itemize{
\item \code{auct}: dataframe containing time-dependent AUC if "auc" was
included as a metric
\item \code{briert}: dataframe containing time-dependent Brier score if "brier" was
included as a metric
}
}
\description{
There are three ways to perform assess the predictive performance:
apparent/internal, bootstrapped, and external. Accordingly, the named list of
prediction models must be as follows:
\itemize{
\item For both apparent/internal evaluation, objects output from
\code{\link[=predict.dynamicLM]{predict.dynamicLM()}} or supermodels fit with \code{\link[=dynamic_lm]{dynamic_lm()}} may be used as input.
\item In order to bootstrap, supermodels fit with \code{\link[=dynamic_lm]{dynamic_lm()}} may be used as input
(note that the argument \code{x=TRUE} must be specified when fitting the model
in \code{\link[=dynamic_lm]{dynamic_lm()}}).
\item For external calibration, supermodels fit with \code{\link[=dynamic_lm]{dynamic_lm()}} are input along
with new data in the \code{data} argument. This data can be a LMdataframe or a
dataframe (in which case \code{lms} must be specified).
}
}
\details{
For both internal evaluation and bootstrapping, it is assumed that all
models in \code{object} are fit on the same data.

If data at late evaluation times is sparse, certain bootstrap samples may
not have patients that live long enough to perform evaluation leading to
the message "Upper limit of followup in bootstrap samples, was too low.
Results at evaluation time(s) beyond these points could not be computed
and are left as NA". In this case, consider only evaluating for earlier
landmarks or performing prediction with a smaller window as data points are
slim. If you wish to see which model/bootstrap/landmark times failed, set
SILENT=FALSE. Set na.rm = TRUE ignores these bootstraps and calculate
metrics from the bootstrap samples that worked (not recommended).

Another message may occur: "Dropping bootstrap b = {X} for model {name} due
to unreliable predictions". As certain approximations are made, numerical
overflow sometimes occurs in predictions for bootstrapped samples. To avoid
potential errors, the whole bootstrap sample is dropped in this case. Note
that input data should be complete otherwise this may occur
unintentionally.
}
\examples{
\dontrun{
# Internal validation
scores <- score(list("Model1" = supermodel),
                times = c(0, 6)) # landmarks at which to provide calibration plots
scores

# Bootstrapping
# Remember to fit the supermodel with argument 'x = TRUE'
scores <- score(list("Model1" = supermodel),
                times = c(0, 6),
                split.method = "bootcv", B = 10) # 10 bootstraps
scores

par(mfrow=c(1,2))
plot(scores)

# External validation
# Either input an object from predict as the object or a supermodel and
# "data" & "lms" argument
newdata <- relapse[relapse$T_txgiven == 0, ]
newdata$age <- newdata$age.at.time.0
newdata$LM <- 0
score(list("CSC" = supermodel), cause = 1, data = newdata, lms = "LM")
}

}
\references{
Paul Blanche, Cecile Proust-Lima, Lucie Loubere, Claudine Berr,
Jean- Francois Dartigues, and Helene Jacqmin-Gadda. Quantifying and
comparing dynamic predictive accuracy of joint models for longitudinal
marker and time-to-event in presence of censoring and competing risks.
Biometrics, 71 (1):102–113, 2015.

P. Blanche, J-F Dartigues, and H. Jacqmin-Gadda. Estimating and comparing
time-dependent areas under receiver operating characteristic curves for
censored event times with competing risks. Statistics in Medicine,
32(30):5381–5397, 2013.
}
